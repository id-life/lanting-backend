import * as fs from "node:fs"
import * as path from "node:path"
import { ArchivesJson } from "./download-archives-json"
import { SearchKeywords } from "./download-search-keywords"

function escapeString(str: string): string {
  return str
    .replace(/'/g, "''")
    .replace(/\\/g, "\\\\")
    .replace(/\n/g, "\\n")
    .replace(/\r/g, "\\r")
}

function generateDataMigrationSQL(): string {
  try {
    // 读取数据文件
    const archivesPath = path.join(__dirname, "../data/archives.json")
    const searchKeywordsPath = path.join(
      __dirname,
      "../data/search-keywords.json",
    )

    if (!fs.existsSync(archivesPath)) {
      throw new Error(`Archives file not found: ${archivesPath}`)
    }

    if (!fs.existsSync(searchKeywordsPath)) {
      throw new Error(`Search keywords file not found: ${searchKeywordsPath}`)
    }

    const archivesData: ArchivesJson = JSON.parse(
      fs.readFileSync(archivesPath, "utf8"),
    )
    const searchKeywordsData: SearchKeywords = JSON.parse(
      fs.readFileSync(searchKeywordsPath, "utf8"),
    )

    let sql = `-- Data migration SQL generated from archives.json and search-keywords.json
-- Generated on: ${new Date().toISOString()}
-- DO NOT EDIT THIS FILE MANUALLY
-- Use "npm run data-migration" to regenerate this file

-- Check if migration has already been run
SET @migration_exists = (SELECT COUNT(*) FROM information_schema.tables 
                        WHERE table_schema = DATABASE() 
                        AND table_name = 'migration_status');

-- Create migration tracking table if it doesn't exist
CREATE TABLE IF NOT EXISTS migration_status (
    id INT PRIMARY KEY AUTO_INCREMENT,
    migration_name VARCHAR(255) UNIQUE NOT NULL,
    executed_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Check if this specific migration has been run
SET @old_data_migrated = (SELECT COUNT(*) FROM migration_status 
                         WHERE migration_name = 'old_data_migration');

-- Only proceed if migration hasn't been run
SET @proceed = IF(@old_data_migrated = 0, 1, 0);

-- Mark migration as started (only if not already done)
INSERT INTO migration_status (migration_name) 
SELECT 'old_data_migration' 
WHERE @proceed = 1;

`

    // 收集所有唯一的实体
    const uniqueAuthors = new Set<string>()
    const uniquePublishers = new Set<string>()
    const uniqueDates = new Set<string>()
    const uniqueTags = new Set<string>()
    const archives = Object.values(archivesData.archives)

    console.log(`Processing ${archives.length} archives...`)

    // 收集唯一实体
    for (const archive of archives) {
      if (archive.author && Array.isArray(archive.author)) {
        archive.author.forEach((author) => {
          if (author && author.trim()) {
            uniqueAuthors.add(author.trim())
          }
        })
      }

      if (archive.publisher && archive.publisher.trim()) {
        uniquePublishers.add(archive.publisher.trim())
      }

      if (archive.date && archive.date.trim()) {
        uniqueDates.add(archive.date.trim())
      }

      if (archive.tag && Array.isArray(archive.tag)) {
        archive.tag.forEach((tag) => {
          if (tag && tag.trim()) {
            uniqueTags.add(tag.trim())
          }
        })
      }
    }

    console.log(
      `Found ${uniqueAuthors.size} unique authors, ${uniquePublishers.size} publishers, ${uniqueDates.size} dates, ${uniqueTags.size} tags`,
    )

    // 插入作者
    if (uniqueAuthors.size > 0) {
      sql += `-- Insert authors (only if migration not done)\n`
      sql += `INSERT INTO authors (name, created_at, updated_at) \n`
      sql += `SELECT * FROM (\n`
      const authorValues = Array.from(uniqueAuthors).map(
        (author) =>
          `    SELECT '${escapeString(author)}' as name, NOW() as created_at, NOW() as updated_at`,
      )
      sql += `${authorValues.join(" UNION ALL\n")}\n`
      sql += `) AS tmp \n`
      sql += `WHERE @proceed = 1 \n`
      sql += `ON DUPLICATE KEY UPDATE name = VALUES(name), updated_at = NOW();\n\n`
    }

    // 插入出版社
    if (uniquePublishers.size > 0) {
      sql += `-- Insert publishers (only if migration not done)\n`
      sql += `INSERT INTO publishers (name, created_at, updated_at) \n`
      sql += `SELECT * FROM (\n`
      const publisherValues = Array.from(uniquePublishers).map(
        (publisher) =>
          `    SELECT '${escapeString(publisher)}' as name, NOW() as created_at, NOW() as updated_at`,
      )
      sql += `${publisherValues.join(" UNION ALL\n")}\n`
      sql += `) AS tmp \n`
      sql += `WHERE @proceed = 1 \n`
      sql += `ON DUPLICATE KEY UPDATE name = VALUES(name), updated_at = NOW();\n\n`
    }

    // 插入日期
    if (uniqueDates.size > 0) {
      sql += `-- Insert dates (only if migration not done)\n`
      sql += `INSERT INTO dates (value, created_at, updated_at) \n`
      sql += `SELECT * FROM (\n`
      const dateValues = Array.from(uniqueDates).map(
        (date) =>
          `    SELECT '${escapeString(date)}' as value, NOW() as created_at, NOW() as updated_at`,
      )
      sql += `${dateValues.join(" UNION ALL\n")}\n`
      sql += `) AS tmp \n`
      sql += `WHERE @proceed = 1 \n`
      sql += `ON DUPLICATE KEY UPDATE value = VALUES(value), updated_at = NOW();\n\n`
    }

    // 插入标签
    if (uniqueTags.size > 0) {
      sql += `-- Insert tags (only if migration not done)\n`
      sql += `INSERT INTO tags (name, created_at, updated_at) \n`
      sql += `SELECT * FROM (\n`
      const tagValues = Array.from(uniqueTags).map(
        (tag) =>
          `    SELECT '${escapeString(tag)}' as name, NOW() as created_at, NOW() as updated_at`,
      )
      sql += `${tagValues.join(" UNION ALL\n")}\n`
      sql += `) AS tmp \n`
      sql += `WHERE @proceed = 1 \n`
      sql += `ON DUPLICATE KEY UPDATE name = VALUES(name), updated_at = NOW();\n\n`
    }

    // 插入档案
    sql += `-- Insert archives (only if migration not done)\n`

    // 分批处理档案以避免SQL过长
    const batchSize = 100
    for (let i = 0; i < archives.length; i += batchSize) {
      const batch = archives.slice(i, i + batchSize)

      sql += `-- Archives batch ${Math.floor(i / batchSize) + 1}\n`
      sql += `INSERT INTO archives (id, title, chapter, remarks, likes, created_at, updated_at) \n`
      sql += `SELECT * FROM (\n`

      const archiveValues = batch.map((archive) => {
        const remarks = archive.remarks
          ? `'${escapeString(archive.remarks)}'`
          : "NULL"
        return `    SELECT ${archive.id} as id, '${escapeString(archive.title)}' as title, '${escapeString(archive.chapter)}' as chapter, ${remarks} as remarks, ${archive.likes} as likes, NOW() as created_at, NOW() as updated_at`
      })

      sql += `${archiveValues.join(" UNION ALL\n")}\n`
      sql += `) AS tmp \n`
      sql += `WHERE @proceed = 1;\n\n`
    }

    // 插入关系数据
    sql += `-- Insert archive relationships (only if migration not done)\n`

    for (const archive of archives) {
      const archiveId = archive.id

      // 作者关系
      if (archive.author && Array.isArray(archive.author)) {
        archive.author.forEach((author, index) => {
          if (author && author.trim()) {
            sql += `-- Archive ${archiveId} "${archive.title}" - Author: ${author}\n`
            sql += `INSERT IGNORE INTO archive_authors (archive_id, author_id, \`order\`) \n`
            sql += `SELECT ${archiveId}, au.id, ${index + 1} \n`
            sql += `FROM authors au \n`
            sql += `WHERE au.name = '${escapeString(author.trim())}' AND @proceed = 1;\n\n`
          }
        })
      }

      // 出版社关系
      if (archive.publisher && archive.publisher.trim()) {
        sql += `-- Archive ${archiveId} "${archive.title}" - Publisher: ${archive.publisher}\n`
        sql += `INSERT IGNORE INTO archive_publishers (archive_id, publisher_id) \n`
        sql += `SELECT ${archiveId}, p.id \n`
        sql += `FROM publishers p \n`
        sql += `WHERE p.name = '${escapeString(archive.publisher.trim())}' AND @proceed = 1;\n\n`
      }

      // 日期关系
      if (archive.date && archive.date.trim()) {
        sql += `-- Archive ${archiveId} "${archive.title}" - Date: ${archive.date}\n`
        sql += `INSERT IGNORE INTO archive_dates (archive_id, date_id) \n`
        sql += `SELECT ${archiveId}, d.id \n`
        sql += `FROM dates d \n`
        sql += `WHERE d.value = '${escapeString(archive.date.trim())}' AND @proceed = 1;\n\n`
      }

      // 标签关系
      if (archive.tag && Array.isArray(archive.tag)) {
        archive.tag.forEach((tag) => {
          if (tag && tag.trim()) {
            sql += `-- Archive ${archiveId} "${archive.title}" - Tag: ${tag}\n`
            sql += `INSERT IGNORE INTO archive_tags (archive_id, tag_id) \n`
            sql += `SELECT ${archiveId}, t.id \n`
            sql += `FROM tags t \n`
            sql += `WHERE t.name = '${escapeString(tag.trim())}' AND @proceed = 1;\n\n`
          }
        })
      }

      // 原始文件
      if (archive.origs && Array.isArray(archive.origs)) {
        archive.origs.forEach((orig) => {
          if (orig && orig.trim()) {
            const fileType = orig.split(".").pop() || "html"
            sql += `-- Archive ${archiveId} "${archive.title}" - Original file: ${orig}\n`
            sql += `INSERT INTO archive_origs (archive_id, storage_url, file_type, storage_type, created_at, updated_at) \n`
            sql += `SELECT ${archiveId}, '${escapeString(orig)}', '${fileType}', 'oss', NOW(), NOW() \n`
            sql += `WHERE @proceed = 1;\n\n`
          }
        })
      }
    }

    // 插入搜索关键词
    const keywords = Object.entries(searchKeywordsData.keywords)
    if (keywords.length > 0) {
      sql += `-- Insert search keywords (only if migration not done)\n`
      sql += `INSERT INTO search_keywords (keyword, search_count, created_at, updated_at) \n`
      sql += `SELECT * FROM (\n`

      const keywordValues = keywords.map(
        ([keyword, count]) =>
          `    SELECT '${escapeString(keyword)}' as keyword, ${count} as search_count, NOW() as created_at, NOW() as updated_at`,
      )

      sql += `${keywordValues.join(" UNION ALL\n")}\n`
      sql += `) AS tmp \n`
      sql += `WHERE @proceed = 1 \n`
      sql += `ON DUPLICATE KEY UPDATE search_count = VALUES(search_count), updated_at = NOW();\n\n`
    }

    sql += `-- Migration completed\n`

    console.log(`Generated SQL with ${keywords.length} search keywords`)

    return sql
  } catch (error) {
    console.error("Failed to generate migration SQL:", error)
    throw error
  }
}

function updateMigrationFile() {
  try {
    // 生成 SQL 文件到 scripts/sql 目录
    const sqlFile = path.join(__dirname, "sql/data_migration.sql")

    // 确保目录存在
    const sqlDir = path.dirname(sqlFile)
    if (!fs.existsSync(sqlDir)) {
      fs.mkdirSync(sqlDir, { recursive: true })
    }

    const sql = generateDataMigrationSQL()

    // 写入 SQL 文件
    fs.writeFileSync(sqlFile, sql)

    console.log(`Migration SQL written to: ${sqlFile}`)
    console.log("You can now execute it with:")
    console.log("  npm run execute-data-migration")
  } catch (error) {
    console.error("Failed to update migration file:", error)
    throw error
  }
}

if (require.main === module) {
  updateMigrationFile()
}

export { generateDataMigrationSQL, updateMigrationFile }
